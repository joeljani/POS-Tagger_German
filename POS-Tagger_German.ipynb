{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagger for the german language using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "universal_tags = tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_tag_list = []\n",
    "for i in range(len(universal_tags)):\n",
    "    universal_tag_list.append(universal_tags[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = open(\"POS_German_train.txt\",\"r+\") \n",
    "test_text = open(\"POS_German_minitest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_text.readlines()\n",
    "test_text = test_text.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_to_sentences(train_text):\n",
    "    sentences = []\n",
    "    for i in range(len(train_text)):\n",
    "        sentences.append(re.sub(r\"([();])\", \"\", train_text[i]))\n",
    "        sentences[i] = re.split(\"\\s\", sentences[i])\n",
    "        sentences[i] = list(filter(None, sentences[i]))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = clean_text_to_sentences(train_text)\n",
    "test_sentences = clean_text_to_sentences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper method\n",
    "def init_list_of_objects(size):\n",
    "    list_of_objects = list()\n",
    "    for i in range(0,size):\n",
    "        list_of_objects.append( list() ) \n",
    "    return list_of_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geht davon aus dass maximal 3 slashes in einem wort stecken... \n",
    "def seperate_words_and_tags(sentences):\n",
    "    sentence_words = init_list_of_objects(len(sentences))\n",
    "    sentence_tags = init_list_of_objects(len(sentences))\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j].count('/') == 2:\n",
    "                sentence_words[i].append(re.split(\"/\", sentences[i][j])[0] + '/' + re.split(\"/\", sentences[i][j])[1])\n",
    "                sentence_tags[i].append(re.split(\"/\", sentences[i][j])[2])\n",
    "            elif sentences[i][j].count('/') == 3:\n",
    "                sentence_words[i].append(re.split(\"/\", sentences[i][j])[0] + '/' + re.split(\"/\", sentences[i][j])[1]\n",
    "                                        + '/' + re.split(\"/\", sentences[i][j])[2])\n",
    "                sentence_tags[i].append(re.split(\"/\", sentences[i][j])[3])\n",
    "            else:\n",
    "                sentence_words[i].append(re.split(\"/\", sentences[i][j])[0])\n",
    "                sentence_tags[i].append(re.split(\"/\", sentences[i][j])[1])\n",
    "    return sentence_words, sentence_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words, sentence_tags = seperate_words_and_tags(sentences)\n",
    "test_sentence_words, test_sentence_tags = seperate_words_and_tags(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset = []\n",
    "for i in range(len(sentence_tags)):\n",
    "    for j in range(len(sentence_tags[i])):\n",
    "        if(sentence_tags[i][j] not in myset):\n",
    "            myset.append(sentence_tags[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset_test = []\n",
    "for i in range(len(test_sentence_tags)):\n",
    "    for j in range(len(test_sentence_tags[i])):\n",
    "        if(test_sentence_tags[i][j] not in myset_test):\n",
    "            myset_test.append(test_sentence_tags[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#should be around 54 -> number of tags in Stuttgart tagset\n",
    "len(myset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuttgart Tagset auf Universal Tagset reduzieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die tags im trainingsset werden auf das Universal Tagset durch ein mapping konvertiert. Folgende Quellen helfen dabei, die verschiedenen Tags auf das Universal Tagset abzustimmen: https://universaldependencies.org/tagset-conversion/de-stts-uposf.html, https://www.nltk.org/_modules/nltk/tag/mapping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_mapping = {'$': '.',\n",
    "                      '$.': '.',\n",
    "                      '$,': '.',\n",
    "                      'NE': 'NOUN',\n",
    "                      'VAFIN': 'VERB',\n",
    "                      'ADV': 'ADV',\n",
    "                      'ART': 'DET',\n",
    "                      'ADJA': 'ADJ',\n",
    "                      'NN': 'NOUN',\n",
    "                      'VVFIN': 'VERB',\n",
    "                      'APPR': 'ADP',\n",
    "                      'PTKVZ': 'ADP',\n",
    "                      'PPOSAT': 'DET',\n",
    "                      'VVPP': 'VERB',\n",
    "                      'FM': 'X',\n",
    "                      'ADJD': 'ADJ',\n",
    "                      'APPRART': 'ADP',\n",
    "                      'KON': 'CONJ',\n",
    "                      'KOUS': 'CONJ',\n",
    "                      'VVINF': 'VERB',\n",
    "                      'VMFIN': 'VERB',\n",
    "                      'PAV': 'ADV',\n",
    "                      'PDAT': 'DET',\n",
    "                      'KOUI': 'CONJ',\n",
    "                      'PTKZU': 'PRT',\n",
    "                      'PIAT': 'DET',\n",
    "                      'PTKNEG': 'PRT',\n",
    "                      'PIS': 'PRON',\n",
    "                      'PRF': 'PRON',\n",
    "                      'CARD': 'NUM',\n",
    "                      'PPER': 'PRON',\n",
    "                      'ITJ': 'PRT',\n",
    "                      'PDS': 'PRON',\n",
    "                      'KOKOM': 'CONJ',\n",
    "                      'PRELS': 'PRON',\n",
    "                      'APPO': 'ADP',\n",
    "                      'PWAT': 'DET',\n",
    "                      'PWAV': 'ADV',\n",
    "                      'VVIZU': 'VERB',\n",
    "                      'PWS': 'PRON',\n",
    "                      'XY': 'X',\n",
    "                      'PRELAT': 'DET',\n",
    "                      'TRUNC': 'X',\n",
    "                      'VAINF': 'VERB',\n",
    "                      'VMINF': 'VERB',\n",
    "                      'VAPP': 'VERB',\n",
    "                      'PTKA': 'PRT',\n",
    "                      'PTKANT': 'PRT',\n",
    "                      'APZR': 'ADP',\n",
    "                      'PPOSS': 'PRON',\n",
    "                      'VVIMP': 'VERB',\n",
    "                      'VAIMP': 'VERB',\n",
    "                      'VMPP': 'VERB',\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stuttgart_tagset_to_universal_tagset(tags):\n",
    "    for i in range(len(tags)):\n",
    "        for index, data in enumerate(tags[i]):\n",
    "            for key, value in conversion_mapping.items():\n",
    "                if key in data:\n",
    "                    tags[i][index] = data.replace(key, conversion_mapping[key])   \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tags = convert_stuttgart_tagset_to_universal_tagset(sentence_tags)\n",
    "test_sentence_tags = convert_stuttgart_tagset_to_universal_tagset(test_sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that there only real tags\n",
    "def filter_correct_tags_only(tags):\n",
    "    tag_lists = init_list_of_objects(len(tags))\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(tags[i])):\n",
    "            if(tags[i][j] in universal_tag_list):\n",
    "                tag_lists[i].append(tags[i][j])\n",
    "    return tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tags = filter_correct_tags_only(sentence_tags)\n",
    "test_sentence_tags = filter_correct_tags_only(test_sentence_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(train_sentences, dev_test_sentences, train_tags, dev_test_tags) = train_test_split(sentence_words, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sätze für Keras Modell vorbereiten (padding + one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique words and tags\n",
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give words and tags a number\n",
    "word_index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word_index['<PAD>'] = 0  \n",
    "word_index['<UNK>'] = 1  \n",
    " \n",
    "tag_index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag_index['<PAD>'] = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X, dev_test_sentences_X, train_tags_y, dev_test_tags_y = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_numbers(sentences):\n",
    "    X = []\n",
    "    for s in sentences:\n",
    "        s_int = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                s_int.append(word_index[w.lower()])\n",
    "            except:\n",
    "                s_int.append(word_index['<UNK>'])\n",
    "        X.append(s_int)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags to numbers\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag_index[t] for t in s])\n",
    "for s in dev_test_tags:\n",
    "    dev_test_tags_y.append([tag_index[t] for t in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X = words_to_numbers(train_sentences)\n",
    "dev_test_sentences_X = words_to_numbers(dev_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5279 44381 63891 36031  3556 53641 11345  6288 53716 11345 22625 11121\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "[ 9 12  4  3  1  1  1  7 10  1  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_test_sentences_X = pad_sequences(dev_test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_test_tags_y = pad_sequences(dev_test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_tags_y_one_hot_encoded = to_categorical(train_tags_y, num_classes=len(tag_index), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 128)          8186496   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 130, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 130, 13)           6669      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 130, 13)           0         \n",
      "=================================================================\n",
      "Total params: 8,981,645\n",
      "Trainable params: 8,981,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word_index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag_index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/3\n",
      "25600/25600 [==============================] - 272s 11ms/step - loss: 0.3479 - acc: 0.8996 - val_loss: 0.1774 - val_acc: 0.9509\n",
      "Epoch 2/3\n",
      "25600/25600 [==============================] - 268s 10ms/step - loss: 0.0677 - acc: 0.9810 - val_loss: 0.0269 - val_acc: 0.9923\n",
      "Epoch 3/3\n",
      "25600/25600 [==============================] - 267s 10ms/step - loss: 0.0158 - acc: 0.9958 - val_loss: 0.0186 - val_acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2027ec50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, train_tags_y_one_hot_encoded, \n",
    "          batch_size=128, \n",
    "          epochs=3, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 28s 3ms/step\n",
      "acc =  acc: 99.35884482860565\n"
     ]
    }
   ],
   "source": [
    "#dev test\n",
    "scores = model.evaluate(dev_test_sentences_X, to_categorical(dev_test_tags_y, len(tag_index)))\n",
    "print(\"acc = \" , f\"{model.metrics_names[1]}: {scores[1] * 100}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung für den finalen Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words_X = words_to_numbers(test_sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_words = pad_sequences(test_words_X, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse\n",
    "def reverse_num_to_tokens(nums, index):\n",
    "    tokens = []\n",
    "    for categoricals in nums:\n",
    "        token = []\n",
    "        for categorical in categoricals:\n",
    "            token.append(index[np.argmax(categorical)])\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = reverse_num_to_tokens(predictions, {i: t for t, i in tag_index.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(tags):\n",
    "    tag_lists = init_list_of_objects(len(tags))\n",
    "    for i in range(len(tags)):\n",
    "        tag_lists[i] = (list(filter(lambda a: a != '<PAD>', tags[i])))\n",
    "    return tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = remove_padding(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct small length errors\n",
    "for i in range(len(predicted_tags)):\n",
    "    while(len(predicted_tags[i]) != len(test_sentence_tags[i])):\n",
    "        if(len(predicted_tags[i]) < len(test_sentence_tags[i])):\n",
    "           test_sentence_tags[i] = test_sentence_tags[i][:-1]\n",
    "        else:\n",
    "           predicted_tags[i] = predicted_tags[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(a_list):\n",
    "    flat_list = [item for sublist in a_list for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags_flattened = flatten(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_tags_flattened = flatten(test_sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.98      0.98      0.98      1178\n",
      "         ADJ       0.93      0.79      0.85       699\n",
      "         ADP       0.95      0.99      0.97      1069\n",
      "         ADV       0.92      0.88      0.90       437\n",
      "        CONJ       0.95      0.89      0.92       308\n",
      "         DET       0.96      0.98      0.97      1055\n",
      "        NOUN       0.91      0.98      0.94      2394\n",
      "         NUM       1.00      0.91      0.96       164\n",
      "        PRON       0.94      0.90      0.92       291\n",
      "         PRT       0.96      0.92      0.94        84\n",
      "        VERB       0.97      0.93      0.95       960\n",
      "           X       0.50      0.10      0.17        20\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8659\n",
      "   macro avg       0.91      0.85      0.87      8659\n",
      "weighted avg       0.94      0.94      0.94      8659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_sentence_tags_flattened, predicted_tags_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy on the final test dataset:  0.9434114793856103  /  94.34 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(test_sentence_tags_flattened, predicted_tags_flattened)\n",
    "print(\"Achieved accuracy on the final test dataset: \" , accuracy_score, \" / \", round((accuracy_score * 100),2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
